{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crucible Python Client Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the Crucible Python client to manage datasets, samples, projects, and their relationships.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Crucible API credentials configured (run `crucible config init` in terminal)\n",
    "- Access to project `crucible-demo`\n",
    "- Example data files in the `data/` directory\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Creating a Sample](#create-sample)\n",
    "3. [Creating a Dataset](#create-dataset)\n",
    "4. [Listing Datasets in a Project](#list-datasets)\n",
    "5. [Getting a Dataset with Metadata](#get-dataset)\n",
    "6. [Updating Dataset Metadata](#update-metadata)\n",
    "7. [Downloading Dataset Files](#download-dataset)\n",
    "8. [Linking a Sample to a Dataset](#link-sample-dataset)\n",
    "9. [Linking Two Datasets (Parent-Child)](#link-datasets)\n",
    "10. [Linking Two Samples (Parent-Child)](#link-samples)\n",
    "11. [Adding a Thumbnail to a Dataset](#add-thumbnail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id='setup'></a>\n## 1. Setup and Configuration\n\nFirst, you need to configure your Crucible API credentials. Run this command in your terminal (only needed once):\n\n```bash\ncrucible config init\n```\n\nThis will prompt you for:\n- **API Key** - Get it from https://crucible.lbl.gov/api/v1/user_apikey\n- **API URL** - Defaults to https://crucible.lbl.gov/api/v1\n- **Default Project** (optional) - You can set `crucible-demo` as default\n\n**Alternative: Without Terminal Access**\n\nIf you don't have terminal access (e.g., JupyterHub, Google Colab, VSCode Flatpak), you can initialize the client directly:\n\n```python\nclient = CrucibleClient(\n    api_url=\"https://crucible.lbl.gov/api/v1\",\n    api_key=\"your-api-key-here\"\n)\n```\n\nOnce configured, you can import and use the Crucible client:"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Crucible client initialized successfully!\n",
      "Data directory: /home/roncofaber/software/nano-crucible/examples/data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from crucible.client import CrucibleClient\n",
    "\n",
    "# Initialize the client (automatically loads configuration)\n",
    "client = CrucibleClient()\n",
    "\n",
    "# Get the data directory path\n",
    "EXAMPLES_DIR = Path(os.getcwd())\n",
    "DATA_DIR = EXAMPLES_DIR / \"data\"\n",
    "\n",
    "print(\"✓ Crucible client initialized successfully!\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your project ID for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Working with project: crucible-demo\n",
      "  Organization: Molecular Foundry\n",
      "  Lead: roncoroni@lbl.gov\n"
     ]
    }
   ],
   "source": [
    "# Project ID for this tutorial\n",
    "PROJECT_ID = \"crucible-demo\"\n",
    "\n",
    "# Verify project exists\n",
    "project = client.projects.get(PROJECT_ID)\n",
    "if project:\n",
    "    print(f\"✓ Working with project: {project['project_id']}\")\n",
    "    print(f\"  Organization: {project.get('organization', 'N/A')}\")\n",
    "    print(f\"  Lead: {project.get('project_lead_email', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"⚠ Project '{PROJECT_ID}' not found. You may need to create it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create-sample'></a>\n",
    "## 2. Creating a Sample\n",
    "\n",
    "Samples represent physical materials or specimens. Let's create a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample created successfully!\n",
      "  Sample ID: 0tcy5e1n1hs53000kqzr2peykr\n",
      "  Sample Name: Silicon Wafer A - Tutorial Example\n",
      "  Project: crucible-demo\n"
     ]
    }
   ],
   "source": [
    "# Create a sample\n",
    "sample = client.samples.create(\n",
    "    sample_name=\"Silicon Wafer A - Tutorial Example\",\n",
    "    project_id=PROJECT_ID,\n",
    "    description=\"Silicon wafer for thermal conductivity measurements (tutorial example)\"\n",
    ")\n",
    "\n",
    "sample_id = sample['unique_id']\n",
    "print(f\"✓ Sample created successfully!\")\n",
    "print(f\"  Sample ID: {sample_id}\")\n",
    "print(f\"  Sample Name: {sample['sample_name']}\")\n",
    "print(f\"  Project: {sample['project_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create-dataset'></a>\n",
    "## 3. Creating a Dataset\n",
    "\n",
    "Datasets contain data files and metadata. You can create a dataset with or without files.\n",
    "\n",
    "### 3.1 Create Dataset with Metadata Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset created successfully!\n",
      "  Dataset ID: 0tcy5tt115xs7000n802q2fxsr\n",
      "  Dataset Name: Thermal Conductivity Measurement - Sample A (Tutorial)\n"
     ]
    }
   ],
   "source": [
    "from crucible.models import BaseDataset\n",
    "\n",
    "# Define dataset metadata\n",
    "dataset_metadata = BaseDataset(\n",
    "    project_id=PROJECT_ID,\n",
    "    measurement=\"thermal_conductivity\",\n",
    "    dataset_name=\"Thermal Conductivity Measurement - Sample A (Tutorial)\",\n",
    "    public=False\n",
    ")\n",
    "\n",
    "# Create dataset without files\n",
    "result = client.datasets.create(\n",
    "    dataset=dataset_metadata,\n",
    "    scientific_metadata={\n",
    "        \"temperature_range\": \"273-363 K\",\n",
    "        \"measurement_method\": \"3-omega method\",\n",
    "        \"equipment\": \"Lakeshore 336 + SR830 lock-in\",\n",
    "        \"sample_type\": \"silicon wafer\"\n",
    "    },\n",
    "    keywords=[\"thermal\", \"conductivity\", \"silicon\", \"tutorial\"]\n",
    ")\n",
    "\n",
    "dataset_id = result['dsid']\n",
    "print(f\"✓ Dataset created successfully!\")\n",
    "print(f\"  Dataset ID: {dataset_id}\")\n",
    "print(f\"  Dataset Name: {result['created_record']['dataset_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Dataset with Files\n",
    "\n",
    "Now let's create a dataset with actual data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to upload:\n",
      "  ✓ thermal_conductivity_data.csv\n",
      "  ✓ measurement_notes.txt\n",
      "\n",
      "✓ Dataset with files created successfully!\n",
      "  Dataset ID: 0tcy5tvd65rjz00039fj0k8nyr\n",
      "  Files uploaded: 2\n",
      "  Ingestion status: complete\n"
     ]
    }
   ],
   "source": [
    "# Create dataset with file upload\n",
    "dataset_with_files = BaseDataset(\n",
    "    project_id=PROJECT_ID,\n",
    "    measurement=\"thermal_conductivity\",\n",
    "    dataset_name=\"Thermal Conductivity Data with Files (Tutorial)\",\n",
    "    public=False\n",
    ")\n",
    "\n",
    "# Files to upload\n",
    "files_to_upload = [\n",
    "    str(DATA_DIR / \"thermal_conductivity_data.csv\"),\n",
    "    str(DATA_DIR / \"measurement_notes.txt\")\n",
    "]\n",
    "\n",
    "# Verify files exist\n",
    "print(\"Files to upload:\")\n",
    "for f in files_to_upload:\n",
    "    exists = \"✓\" if Path(f).exists() else \"✗\"\n",
    "    print(f\"  {exists} {Path(f).name}\")\n",
    "\n",
    "result_with_files = client.datasets.create(\n",
    "    dataset=dataset_with_files,\n",
    "    files_to_upload=files_to_upload,\n",
    "    scientific_metadata={\n",
    "        \"temperature_range\": \"273-363 K\",\n",
    "        \"data_points\": 10,\n",
    "        \"measurement_method\": \"3-omega method\",\n",
    "        \"sample_material\": \"silicon\"\n",
    "    },\n",
    "    keywords=[\"thermal\", \"conductivity\", \"data\", \"tutorial\"],\n",
    "    ingestor=\"ApiUploadIngestor\",\n",
    "    wait_for_ingestion_response=True\n",
    ")\n",
    "\n",
    "dataset_with_files_id = result_with_files['dsid']\n",
    "print(f\"\\n✓ Dataset with files created successfully!\")\n",
    "print(f\"  Dataset ID: {dataset_with_files_id}\")\n",
    "print(f\"  Files uploaded: {len(result_with_files.get('uploaded_files', []))}\")\n",
    "print(f\"  Ingestion status: {result_with_files.get('ingestion_request', {}).get('status', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='list-datasets'></a>\n",
    "## 4. Listing Datasets in a Project\n",
    "\n",
    "Retrieve all datasets associated with a project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 dataset(s) in project crucible-demo\n",
      "\n",
      "1. 0tcy5mfwcxyxs000fs84n0vacw\n",
      "   Name: Thermal Conductivity Measurement - Sample A (Tutorial)\n",
      "   Measurement: thermal_conductivity\n",
      "   Public: False\n",
      "   Created: 2026-02-24\n",
      "\n",
      "2. 0tcy5n3hpnxhn0001scz149deg\n",
      "   Name: Thermal Conductivity Data with Files (Tutorial)\n",
      "   Measurement: thermal_conductivity\n",
      "   Public: False\n",
      "   Created: 2026-02-24\n",
      "\n",
      "3. 0tcy5q5ytsvf1000h9jaz9kfc8\n",
      "   Name: Processed Thermal Conductivity Data (Tutorial)\n",
      "   Measurement: thermal_conductivity_analysis\n",
      "   Public: False\n",
      "   Created: 2026-02-24\n",
      "\n",
      "4. 0tcy5tt115xs7000n802q2fxsr\n",
      "   Name: Thermal Conductivity Measurement - Sample A (Tutorial)\n",
      "   Measurement: thermal_conductivity\n",
      "   Public: False\n",
      "   Created: 2026-02-24\n",
      "\n",
      "5. 0tcy5tvd65rjz00039fj0k8nyr\n",
      "   Name: Thermal Conductivity Data with Files (Tutorial)\n",
      "   Measurement: thermal_conductivity\n",
      "   Public: False\n",
      "   Created: 2026-02-24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all datasets in the project\n",
    "datasets = client.datasets.list(project_id=PROJECT_ID, limit=50)\n",
    "\n",
    "print(f\"Found {len(datasets)} dataset(s) in project {PROJECT_ID}\\n\")\n",
    "\n",
    "# Display first 5 datasets\n",
    "for i, ds in enumerate(datasets[:5], 1):\n",
    "    print(f\"{i}. {ds.get('unique_id', 'N/A')}\")\n",
    "    print(f\"   Name: {ds.get('dataset_name', 'Unnamed')}\")\n",
    "    print(f\"   Measurement: {ds.get('measurement', 'N/A')}\")\n",
    "    print(f\"   Public: {ds.get('public', False)}\")\n",
    "    if ds.get('creation_time'):\n",
    "        print(f\"   Created: {ds['creation_time'][:10]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='get-dataset'></a>\n",
    "## 5. Getting a Dataset with Metadata\n",
    "\n",
    "Retrieve detailed information about a specific dataset, including scientific metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 0tcy5tt115xs7000n802q2fxsr\n",
      "Name: Thermal Conductivity Measurement - Sample A (Tutorial)\n",
      "Measurement: thermal_conductivity\n",
      "Public: False\n",
      "Project: crucible-demo\n",
      "\n",
      "Scientific Metadata:\n",
      "  id: 107978\n",
      "  dataset_unique_id: 0tcy5tt115xs7000n802q2fxsr\n",
      "  scientific_metadata: {'temperature_range': '273-363 K', 'measurement_method': '3-omega method', 'equipment': 'Lakeshore 336 + SR830 lock-in', 'sample_type': 'silicon wafer'}\n"
     ]
    }
   ],
   "source": [
    "# Get dataset with metadata\n",
    "dataset_details = client.datasets.get(\n",
    "    dsid=dataset_id,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {dataset_details['unique_id']}\")\n",
    "print(f\"Name: {dataset_details.get('dataset_name', 'N/A')}\")\n",
    "print(f\"Measurement: {dataset_details.get('measurement', 'N/A')}\")\n",
    "print(f\"Public: {dataset_details.get('public', False)}\")\n",
    "print(f\"Project: {dataset_details.get('project_id', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nScientific Metadata:\")\n",
    "if 'scientific_metadata' in dataset_details and dataset_details['scientific_metadata']:\n",
    "    for key, value in dataset_details['scientific_metadata'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"  No metadata available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get keywords and other dataset properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: thermal, silicon, conductivity, tutorial\n",
      "Number of thumbnails: 0\n"
     ]
    }
   ],
   "source": [
    "# Get keywords\n",
    "keywords = client.datasets.get_keywords(dataset_id)\n",
    "if keywords:\n",
    "    keyword_list = [kw.get('keyword', '') for kw in keywords]\n",
    "    print(f\"Keywords: {', '.join(keyword_list)}\")\n",
    "else:\n",
    "    print(\"Keywords: None\")\n",
    "\n",
    "# Get thumbnails\n",
    "thumbnails = client.datasets.get_thumbnails(dataset_id)\n",
    "print(f\"Number of thumbnails: {len(thumbnails)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='update-metadata'></a>\n",
    "## 6. Updating Dataset Metadata\n",
    "\n",
    "You can update scientific metadata for an existing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update scientific metadata for the dataset\n",
    "updated_metadata = {\n",
    "    \"temperature_range\": \"273-363 K\",\n",
    "    \"measurement_method\": \"3-omega method\",\n",
    "    \"equipment\": \"Lakeshore 336 + SR830 lock-in\",\n",
    "    \"sample_type\": \"silicon wafer\",\n",
    "    \"calibration_date\": \"2026-02-20\",\n",
    "    \"operator\": \"Tutorial User\"\n",
    "}\n",
    "\n",
    "result = client.datasets.update_scientific_metadata(\n",
    "    dsid=dataset_id,\n",
    "    metadata=updated_metadata\n",
    ")\n",
    "\n",
    "print(f\"✓ Scientific metadata updated for dataset {dataset_id}\")\n",
    "\n",
    "# Verify the update\n",
    "dataset_updated = client.datasets.get(dsid=dataset_id, include_metadata=True)\n",
    "print(f\"\\nUpdated Scientific Metadata:\")\n",
    "if 'scientific_metadata' in dataset_updated and dataset_updated['scientific_metadata']:\n",
    "    sci_meta = dataset_updated['scientific_metadata'].get('scientific_metadata', {})\n",
    "    for key, value in sci_meta.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='download-dataset'></a>\n",
    "## 7. Downloading Dataset Files\n",
    "\n",
    "You can download files from datasets that have been ingested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download links for dataset 0tcy7dca5xsxv000f5zq5n047r:\n",
      "\n",
      "  File: 0tcy7dca5xsxv000f5zq5n047r/0tcy7dca5xsxv000f5zq5n047r_ingest_2026-02-24T155923.110037-0800_19079.json\n",
      "  URL: https://storage.googleapis.com/mf-storage-prod/0tcy7dca5xsxv000f5zq5n047r/0tcy7d...\n",
      "\n",
      "  File: 0tcy7dca5xsxv000f5zq5n047r/measurement_notes.txt\n",
      "  URL: https://storage.googleapis.com/mf-storage-prod/0tcy7dca5xsxv000f5zq5n047r/measur...\n",
      "\n",
      "  File: 0tcy7dca5xsxv000f5zq5n047r/thermal_conductivity_data.csv\n",
      "  URL: https://storage.googleapis.com/mf-storage-prod/0tcy7dca5xsxv000f5zq5n047r/therma...\n",
      "\n",
      "Downloading files to: /tmp/tmpzvt_p1b_\n",
      "\n",
      "✓ Downloaded 3 file(s):\n",
      "  - 0tcy7dca5xsxv000f5zq5n047r_ingest_2026-02-24T155923.110037-0800_19079.json\n",
      "  - measurement_notes.txt\n",
      "  - thermal_conductivity_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Get download links for dataset files\n",
    "download_links = client.datasets.get_download_links(dataset_with_files_id)\n",
    "\n",
    "print(f\"Download links for dataset {dataset_with_files_id}:\\n\")\n",
    "for file_path, url in download_links.items():\n",
    "    print(f\"  File: {file_path}\")\n",
    "    print(f\"  URL: {url[:80]}...\")\n",
    "    print()\n",
    "\n",
    "# Download all files from the dataset to a directory\n",
    "import tempfile\n",
    "download_dir = Path(tempfile.mkdtemp())\n",
    "\n",
    "print(f\"Downloading files to: {download_dir}\")\n",
    "downloaded_files = client.datasets.download(\n",
    "    dsid=dataset_with_files_id,\n",
    "    output_dir=str(download_dir)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Downloaded {len(downloaded_files)} file(s):\")\n",
    "for file_path in downloaded_files:\n",
    "    print(f\"  - {Path(file_path).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='link-sample-dataset'></a>\n",
    "## 8. Linking a Sample to a Dataset\n",
    "\n",
    "Associate a dataset with a sample to indicate which sample the data comes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample 0tcy5e1n1hs53000kqzr2peykr linked to dataset 0tcy7d8txhvw300084w1cxdbp0\n",
      "\n",
      "Datasets linked to sample 0tcy5e1n1hs53000kqzr2peykr: 3\n",
      "  - 0tcy5mfwcxyxs000fs84n0vacw: Thermal Conductivity Measurement - Sample A (Tutorial)\n",
      "  - 0tcy5tt115xs7000n802q2fxsr: Thermal Conductivity Measurement - Sample A (Tutorial)\n",
      "  - 0tcy7d8txhvw300084w1cxdbp0: Thermal Conductivity Measurement - Sample A (Tutorial)\n"
     ]
    }
   ],
   "source": [
    "# Link sample to dataset\n",
    "result = client.samples.add_to_dataset(\n",
    "    sample_id=sample_id,\n",
    "    dataset_id=dataset_id\n",
    ")\n",
    "\n",
    "print(f\"✓ Sample {sample_id} linked to dataset {dataset_id}\")\n",
    "\n",
    "# Verify the link\n",
    "datasets_for_sample = client.datasets.list(sample_id=sample_id)\n",
    "print(f\"\\nDatasets linked to sample {sample_id}: {len(datasets_for_sample)}\")\n",
    "for ds in datasets_for_sample:\n",
    "    print(f\"  - {ds['unique_id']}: {ds.get('dataset_name', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='link-datasets'></a>\n",
    "## 9. Linking Two Datasets (Parent-Child)\n",
    "\n",
    "Create hierarchical relationships between datasets. For example, link a processed dataset to its raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second dataset (processed data)\n",
    "processed_dataset = BaseDataset(\n",
    "    project_id=PROJECT_ID,\n",
    "    measurement=\"thermal_conductivity_analysis\",\n",
    "    dataset_name=\"Processed Thermal Conductivity Data (Tutorial)\",\n",
    "    public=False\n",
    ")\n",
    "\n",
    "result_processed = client.datasets.create(\n",
    "    dataset=processed_dataset,\n",
    "    scientific_metadata={\n",
    "        \"thermal_conductivity_300K\": 148.5,\n",
    "        \"thermal_conductivity_unit\": \"W/m·K\",\n",
    "        \"processing_method\": \"polynomial curve fitting (order 2)\",\n",
    "        \"uncertainty\": 1.8,\n",
    "        \"parent_dataset\": dataset_with_files_id\n",
    "    },\n",
    "    keywords=[\"processed\", \"thermal\", \"conductivity\", \"analysis\", \"tutorial\"]\n",
    ")\n",
    "\n",
    "processed_dataset_id = result_processed['dsid']\n",
    "print(f\"✓ Processed dataset created: {processed_dataset_id}\")\n",
    "\n",
    "# Link datasets: raw data (parent) -> processed data (child)\n",
    "link_result = client.datasets.link_parent_child(\n",
    "    parent_dataset_id=dataset_with_files_id,\n",
    "    child_dataset_id=processed_dataset_id\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Datasets linked successfully!\")\n",
    "print(f\"  Parent (raw data): {dataset_with_files_id}\")\n",
    "print(f\"  Child (processed): {processed_dataset_id}\")\n",
    "\n",
    "# List children of parent dataset\n",
    "children = client.datasets.list_children(dataset_with_files_id)\n",
    "print(f\"\\nChild datasets of {dataset_with_files_id}: {len(children)}\")\n",
    "for child in children:\n",
    "    print(f\"  - {child['unique_id']}: {child.get('dataset_name', 'N/A')}\")\n",
    "\n",
    "# List parents of child dataset\n",
    "parents = client.datasets.list_parents(processed_dataset_id)\n",
    "print(f\"\\nParent datasets of {processed_dataset_id}: {len(parents)}\")\n",
    "for parent in parents:\n",
    "    print(f\"  - {parent['unique_id']}: {parent.get('dataset_name', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subsample\n",
    "subsample = client.samples.create(\n",
    "    sample_name=\"Silicon Wafer A - Region 1 (Tutorial)\",\n",
    "    project_id=PROJECT_ID,\n",
    "    description=\"Sub-region of wafer A for localized measurements (tutorial example)\"\n",
    ")\n",
    "\n",
    "subsample_id = subsample['unique_id']\n",
    "print(f\"✓ Subsample created: {subsample_id}\")\n",
    "\n",
    "# Link samples: parent sample -> subsample\n",
    "link_result = client.samples.link(\n",
    "    parent_id=sample_id,\n",
    "    child_id=subsample_id\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Samples linked successfully!\")\n",
    "print(f\"  Parent: {sample_id}\")\n",
    "print(f\"  Child: {subsample_id}\")\n",
    "\n",
    "# List children of parent sample\n",
    "children = client.samples.list_children(sample_id)\n",
    "print(f\"\\nChild samples of {sample_id}: {len(children)}\")\n",
    "for child in children:\n",
    "    print(f\"  - {child['unique_id']}: {child.get('sample_name', 'N/A')}\")\n",
    "\n",
    "# List parents of child sample\n",
    "parents = client.samples.list_parents(subsample_id)\n",
    "print(f\"\\nParent samples of {subsample_id}: {len(parents)}\")\n",
    "for parent in parents:\n",
    "    print(f\"  - {parent['unique_id']}: {parent.get('sample_name', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='link-samples'></a>\n",
    "## 10. Linking Two Samples (Parent-Child)\n",
    "\n",
    "Create hierarchical relationships between samples. For example, link a subsample to its parent sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to thumbnail image\n",
    "thumbnail_path = str(DATA_DIR / \"thermal_measurement_preview.png\")\n",
    "\n",
    "# Verify file exists\n",
    "if Path(thumbnail_path).exists():\n",
    "    print(f\"✓ Thumbnail file found: {Path(thumbnail_path).name}\")\n",
    "    \n",
    "    # Add thumbnail to dataset\n",
    "    result = client.datasets.add_thumbnail(\n",
    "        dsid=dataset_id,\n",
    "        file_path=thumbnail_path,\n",
    "        thumbnail_name=\"thermal_measurement_preview\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Thumbnail added to dataset {dataset_id}\")\n",
    "    \n",
    "    # List all thumbnails for the dataset\n",
    "    thumbnails = client.datasets.get_thumbnails(dataset_id)\n",
    "    print(f\"\\nThumbnails for dataset:\")\n",
    "    for thumb in thumbnails:\n",
    "        print(f\"  - {thumb.get('name', 'unnamed')}\")\n",
    "else:\n",
    "    print(f\"✗ Thumbnail file not found: {thumbnail_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='add-thumbnail'></a>\n",
    "## 11. Adding a Thumbnail to a Dataset\n",
    "\n",
    "Upload a thumbnail image to provide a visual preview of your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all IDs created\n",
    "print(\"Resource IDs created in this tutorial:\")\n",
    "print(f\"\\nSamples:\")\n",
    "print(f\"  sample_id = {sample_id}\")\n",
    "print(f\"  subsample_id = {subsample_id}\")\n",
    "print(f\"\\nDatasets:\")\n",
    "print(f\"  dataset_id = {dataset_id}\")\n",
    "print(f\"  dataset_with_files_id = {dataset_with_files_id}\")\n",
    "print(f\"  processed_dataset_id = {processed_dataset_id}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"You can open any of these resources in your browser using:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n  crucible open {sample_id}\")\n",
    "print(f\"  crucible open {dataset_id}\")\n",
    "print(f\"  crucible open {dataset_with_files_id}\")\n",
    "print(\"\\nThe 'crucible open' command works with any dataset, sample, or project ID.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the core Crucible operations:\n",
    "\n",
    "1. ✓ **Configuration** - Set up API credentials with `crucible config init`\n",
    "2. ✓ **Create Sample** - `client.samples.create()`\n",
    "3. ✓ **Create Dataset** - `client.datasets.create()` with optional files\n",
    "4. ✓ **List Datasets** - `client.datasets.list(project_id=...)`\n",
    "5. ✓ **Get Dataset Details** - `client.datasets.get(dsid=..., include_metadata=True)`\n",
    "6. ✓ **Update Dataset Metadata** - `client.datasets.update_scientific_metadata()`\n",
    "7. ✓ **Download Dataset Files** - `client.datasets.get_download_links()` and `client.datasets.download()`\n",
    "8. ✓ **Link Sample to Dataset** - `client.samples.add_to_dataset()`\n",
    "9. ✓ **Link Datasets** - `client.datasets.link_parent_child()`, `list_children()`, `list_parents()`\n",
    "10. ✓ **Link Samples** - `client.samples.link()`, `list_children()`, `list_parents()`\n",
    "11. ✓ **Add Thumbnail** - `client.datasets.add_thumbnail()`\n",
    "\n",
    "### Resource IDs Created in This Tutorial\n",
    "\n",
    "The following variables contain IDs of resources created in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in project: 2\n",
      "Total accessible projects: 187\n"
     ]
    }
   ],
   "source": [
    "# Update dataset metadata\n",
    "# client.datasets.update_scientific_metadata(\n",
    "#     dataset_id, \n",
    "#     {\"new_field\": \"value\", \"updated_field\": \"new_value\"}\n",
    "# )\n",
    "\n",
    "# Add additional keywords\n",
    "# client.datasets.add_keyword(dataset_id, \"new-keyword\")\n",
    "\n",
    "# Upload additional files to existing dataset\n",
    "# client.datasets.upload_file(dataset_id, \"/path/to/file.txt\")\n",
    "\n",
    "# Request SciCat upload\n",
    "# client.datasets.request_scicat_upload(dataset_id)\n",
    "\n",
    "# List all samples in a project\n",
    "samples = client.samples.list(project_id=PROJECT_ID, limit=999999)\n",
    "print(f\"Total samples in project: {len(samples)}\")\n",
    "\n",
    "# List all projects\n",
    "projects = client.projects.list(limit=9999)\n",
    "print(f\"Total accessible projects: {len(projects)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "- **Documentation**: See `crucible/cli/README.md` for CLI usage\n",
    "- **API Reference**: Check docstrings in `crucible/resources/` for all available methods\n",
    "- **Examples**: More examples in `crucible/parsers/` for specialized data formats (LAMMPS, MatEnsemble)\n",
    "- **Data Files**: Example data files used in this tutorial are in `examples/data/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10k",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}